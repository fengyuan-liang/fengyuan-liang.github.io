# 一文读懂AB测试完整流程

![1](http://www.appadhoc.com/blog/wp-content/uploads/2021/12/1.png)



​	上图即为一次完整的AB测试流程，看似步骤简单，但AB测试本身就是精耕细作的过程，任何一个环节出现问题都会影响最终的结果，导致我们对业务结果的错误预判，细节决定了AB测试的结果是否准确、可用。所以本期我们来深入探讨试验过程中的一些注意事项，避免落入试验陷阱。

#### 一、确定指标

当我们准备做AB测试前，我们首先要清楚为什么要做AB测试。AB测试不同于其他用户行为分析工具，二者的区别在于后者注重“面面俱到”，即通过大量细致的埋点或圈选，让我们了解到每个用户在产品上的具体行为，比如在哪个页面浏览了，哪个按钮点击了，哪个页面停留时间比较长等等。而当产品或运营同学拿到这些数据后，总会发现一些指标的效果是不尽人意的，那这些数据我们应该如何有效提升呢？所以这里就要提到AB测试了，当我们发现某个指标数据不如业内平均水平或者没有达到预定KPI时，我们就可以把这个指标设置为本次试验的关键指标，然后围绕这个指标做出合理假设，推进试验。

在热云数据AppAdhoc A/B Testing平台，指标分为两种：

**l 关键指标**

**l 辅助指标**

关键指标即为在本次试验中我们关注的最核心的北极星指标，可以根据产品所处阶段或者拿到的待优化数据进行选取；一般情况下关键指标只有一个，但在实际业务中指标都是相关联的，我们不能为了短期的关键目标而影响长期目标，所以这时我们就可以再设置几个辅助指标帮助我们进行业务决策。

####  二、 产生想法

确定试验指标后，我们就可以围绕指标思考如何对页面做出改动。产生想法的途径有很多：比如日常的头脑风暴，用户的反馈意见等，只要想法是合理的，我们都可以利用AB测试进行尝试。例如下图的改版方案包含很多试验设计：

![2](http://www.appadhoc.com/blog/wp-content/uploads/2021/12/2.png)

#### 三、设计试验

既然确定了指标，也有了改动的想法，那接下来我们就可以据此生成试验版本，设计试验了。设计试验过程中要注意以下几点：

**l AB测试谁来做**

AB测试是一件需要团队协作的工作，不是一个人能够完成的。比如数据收集和分析的部分需要产品或运营同学，SDK集成部分需要开发同学，版本设计需要UI同学等等。总之AB测试需要各部门之间的配合协作，一起完成试验。

**l 样本量的确定**

关于样本量的确定，之前的文章中我们有介绍过。在热云数据AppAdhoc A/B Testing平台我们会提供“[样本计算器](https://mp.weixin.qq.com/s?__biz=MzAxNzYxNjE1OA==&mid=2650278090&idx=1&sn=53202f2a48e75f95717b2f86974ac5b4&scene=21#wechat_redirect)”的小工具，无论是大流量还是小流量，都可以通过这个工具计算出试验所需的样本量以及试验所需的时间，方便快捷。

**l 版本优劣的判断**

正常情况下，我们都是以“指标转化提升”为最终目标的，即正向指标。实际上有些业务场景，会有观测负向指标的情况。

例如，在上线了新的功能之后，将”问题反馈”（feedback）作为关键指标，希望测试新功能是否会产生一些未知BUG。此时该指标为负向指标，即反馈率越低越好，因此，在观测试验数据的时候，该指标的置信区间在试验版本中的表现在下图中的表现均为负向收敛。说明新版本上线后并没有造成问题反馈增加的情况（需要特别注意，不能将新功能上线作为反馈率更低的根本原因，还需要根据试验的具体运营情况得出更准确的判断，此处仅为示例）

![3](http://www.appadhoc.com/blog/wp-content/uploads/2021/12/3.png)

#### 四、 集成上线

集成上线部分的主要工作就是集成SDK（或API）和后台配置。

**l 试验模式**

热云数据AppAdhoc A/B Testing平台提供三种试验模式：

**a. 可视化模式：**可视化模式只需集成SDK，就可以在A/B平台完成试验版本编辑修改、设定优化指标，省去代码工作量，大大提高试验效率（适用于UI层面的调整）。

**b. 编程模式：**该模式试验提供更多自定义的功能，需要代码集成，但编程模式能满足各种复杂的试验场景，适用范围广泛。

**c. 多链接合并模式：**该模式可以将多个URL通过投放唯一的原始版本URL，将访问原始版本URL的用户自动分流到各个试验页面URL中，并获取样本和指标数据，验证哪个URL页面转化率更高，适用于着陆页推广的场景。

**l 调整流量**

关于科学分流。不同版本之间的流量分配务必保证随机均匀。在以往的试验中，有些用户希望某一版本的数据尽快增长，试验运行1-2天后感觉某一版本指标转化率达到增长预期，只提高了某一版本的流量，而保持其他版本的流量不变。从而打破了版本流量比例均衡的原则，这样做的后果就是分流不均，试验数据毫无参考价值。

AppAdhoc A/B Testing平台在客户进行A/B试验的同时，也支持客户进行灰度发布，因此在底层设计的时候就允许用户按需调整流量，在随机且均匀的A/B测试进行中，发现了稳定增长的版本后，开始逐步提高获胜版本的流量比例，直至100%推送版本，在此期间观察用户反馈，如果没有异常反馈，就可以选择一键推送来实现全量发布。

#### 五、 分析数据

试验结束后，我们通常根据置信区间是否收敛，统计功效是否显著等判断置信区间的表现。在热云数据AppAdhoc A/B Testing平台，置信区间有三种结果：

![4](http://www.appadhoc.com/blog/wp-content/uploads/2021/12/4.png)

通过置信区间的表现，我们可以判断试验指标的转化效果如何，是否可以推送至全量用户。而一般能够影响统计功效的就是流量是否充足，有时虽然置信区间收敛，但功效是不足的，这就说明参与试验的样本量不足，需要再跑一段时间收集更多的样本量来达到功效足够的效果。

以上即为AB测试的完整流程，虽然试验过程是复杂的，但却实实在在能够带来价值。AB测试给了公司内部决策者一副“公正的耳朵”，帮助大家听取用户的诉求并做到用数据驱动决策。